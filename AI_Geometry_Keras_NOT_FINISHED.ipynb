{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras for Geometry dash\n",
    "\n",
    "This time we are going to use Keras-RL for the same project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow was built with CUDA\n",
      "GPUs available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Lets see if CUDA is available\n",
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"TensorFlow was built with CUDA\")\n",
    "else:\n",
    "    print(\"TensorFlow was not built with CUDA\")\n",
    "\n",
    "print(\"GPUs available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "\n",
    "global green_per\n",
    "green_per = 0.1\n",
    "def capture_screen(region=(0, 0, 640, 480), resize_dim=(160, 120)):\n",
    "    global green_per\n",
    "\n",
    "    screen = pyautogui.screenshot(region=region)\n",
    "    \n",
    "    # Convert screenshot into numpy array\n",
    "    frame = np.array(screen, dtype=np.uint8)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to HSV\n",
    "    frame_hsv = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "        # Define range of green color in HSV\n",
    "    lower_green = np.array([50, 100, 128])\n",
    "    upper_green = np.array([70, 255, 255])\n",
    "    # Adjust these values according to your needs\n",
    "    \n",
    "    # Threshold the HSV image to get only green colors\n",
    "    mask = cv2.inRange(frame_hsv, lower_green, upper_green)\n",
    "    \n",
    "    # Calculate the percentage of green pixels\n",
    "    green_per = np.sum(mask == 255) / (mask.size) * 100\n",
    "    \n",
    "    # Resize image\n",
    "    frame_resized = cv2.resize(frame_rgb, resize_dim)\n",
    "    \n",
    "    return frame_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Gym Enviroment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "q\n",
    "class GeometryDashEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(GeometryDashEnv, self).__init__()\n",
    "        # Define el espacio de observación (estado del juego)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(160, 120, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Define el espacio de acciones\n",
    "        self.action_space = spaces.Discrete(2)  # 0: do nothing, 1: jump\n",
    "        \n",
    "        # Inicializa el porcentaje de verde (para detectar si el cubo está vivo)\n",
    "        self.green_per = 0.1\n",
    "    \n",
    "\n",
    "    def capture_screen(self, region=(0, 0, 640, 480), resize_dim=(160, 120)):\n",
    "        screen = pyautogui.screenshot(region=region)\n",
    "        frame = np.array(screen, dtype=np.uint8)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convertir a HSV y encontrar el porcentaje de verde\n",
    "        frame_hsv = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2HSV)\n",
    "        lower_green = np.array([50, 100, 128])\n",
    "        upper_green = np.array([70, 255, 255])\n",
    "        mask = cv2.inRange(frame_hsv, lower_green, upper_green)\n",
    "        self.green_per = np.sum(mask == 255) / mask.size * 100\n",
    "        \n",
    "        # Redimensionar imagen\n",
    "        frame_resized = cv2.resize(frame_rgb, resize_dim)\n",
    "        return frame_resized\n",
    "\n",
    "    def reset(self):\n",
    "        return self.capture_screen()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Realizar acción\n",
    "        if action == 1:\n",
    "            pyautogui.click()\n",
    "        \n",
    "        # Capturar el nuevo estado\n",
    "        self.state = self.capture_screen()\n",
    "        \n",
    "        done = False\n",
    "        reward = 1  # Recompensa por seguir vivo\n",
    "        \n",
    "        # Si el porcentaje de verde es muy bajo, podría indicar que el cubo ha muerto\n",
    "        if self.green_per == 0: \n",
    "            done = True\n",
    "            reward = -50   # Penalización por morir\n",
    "        \n",
    "        info = {'green_percentage': self.green_per}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        display_img = self.state.copy()  # Haz una copia para no modificar el estado original\n",
    "        \n",
    "        if mode == 'human':\n",
    "            # Dibuja texto con el porcentaje de verde detectado\n",
    "            cv2.putText(display_img, f'Green %: {self.green_per:.2f}', (10, 20), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            \n",
    "            cv2.imshow('Gameplay with Stats', display_img)\n",
    "            cv2.waitKey(1)\n",
    "        elif mode == 'rgb_array':\n",
    "            return display_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_2_input to have 4 dimensions, but got array with shape (1, 1, 120, 160, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m dqn\u001b[38;5;241m.\u001b[39mcompile(Adam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Test the trained agent\u001b[39;00m\n\u001b[0;32m     36\u001b[0m dqn\u001b[38;5;241m.\u001b[39mtest(env, nb_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32me:\\AI_FOR_Geometry_dash\\.venv\\lib\\site-packages\\rl\\core.py:168\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    165\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_step_begin(episode_step)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# This is were all of the work happens. We first perceive and compute the action\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# (forward step) and then use the reward to improve (backward step).\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mprocess_action(action)\n",
      "File \u001b[1;32me:\\AI_FOR_Geometry_dash\\.venv\\lib\\site-packages\\rl\\agents\\dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# Select an action.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mget_recent_state(observation)\n\u001b[1;32m--> 224\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m    226\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mselect_action(q_values\u001b[38;5;241m=\u001b[39mq_values)\n",
      "File \u001b[1;32me:\\AI_FOR_Geometry_dash\\.venv\\lib\\site-packages\\rl\\agents\\dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_q_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m---> 68\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_batch_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions,)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32me:\\AI_FOR_Geometry_dash\\.venv\\lib\\site-packages\\rl\\agents\\dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_batch_q_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_batch):\n\u001b[0;32m     62\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[1;32m---> 63\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mlen\u001b[39m(state_batch), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32me:\\AI_FOR_Geometry_dash\\.venv\\lib\\site-packages\\keras\\engine\\training_v1.py:1304\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`predict_on_batch` is not supported for models distributed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith tf.distribute.Strategy.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1302\u001b[0m     )\n\u001b[0;32m   1303\u001b[0m \u001b[38;5;66;03m# Validate and standardize user data.\u001b[39;00m\n\u001b[1;32m-> 1304\u001b[0m inputs, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_tensors_from_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   1306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;66;03m# If `self._distribution_strategy` is True, then we are in a replica\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;66;03m# context at this point.\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_eagerly \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy:\n",
      "File \u001b[1;32me:\\AI_FOR_Geometry_dash\\.venv\\lib\\site-packages\\keras\\engine\\training_v1.py:2649\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2641\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2642\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2645\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(_is_symbolic_tensor(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_inputs)\n\u001b[0;32m   2646\u001b[0m ):\n\u001b[0;32m   2647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2649\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2651\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\AI_FOR_Geometry_dash\\.venv\\lib\\site-packages\\keras\\engine\\training_v1.py:2690\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2687\u001b[0m \u001b[38;5;66;03m# Standardize the inputs.\u001b[39;00m\n\u001b[0;32m   2688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset)):\n\u001b[0;32m   2689\u001b[0m     \u001b[38;5;66;03m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[1;32m-> 2690\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_input_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeed_input_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeed_input_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2698\u001b[0m \u001b[38;5;66;03m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[0;32m   2699\u001b[0m \u001b[38;5;66;03m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[0;32m   2700\u001b[0m \u001b[38;5;66;03m# at all times - validate that this is so and refactor the\u001b[39;00m\n\u001b[0;32m   2701\u001b[0m \u001b[38;5;66;03m# standardization code.\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n",
      "File \u001b[1;32me:\\AI_FOR_Geometry_dash\\.venv\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:714\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    712\u001b[0m shape \u001b[38;5;241m=\u001b[39m shapes[i]\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape):\n\u001b[1;32m--> 714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    715\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when checking \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;241m+\u001b[39m exception_prefix\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;241m+\u001b[39m names[i]\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(shape))\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dimensions, but got array \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith shape \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(data_shape)\n\u001b[0;32m    723\u001b[0m     )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_batch_axis:\n\u001b[0;32m    725\u001b[0m     data_shape \u001b[38;5;241m=\u001b[39m data_shape[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_2_input to have 4 dimensions, but got array with shape (1, 1, 120, 160, 3)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger\n",
    "\n",
    "\n",
    "num_actions = 2 # jump or not jump\n",
    "path = './model.keras'\n",
    "\n",
    "env = GeometryDashEnv()\n",
    "input_shape = (120, 160, 3)\n",
    "# Define your CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "cnn_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dense(num_actions, activation='linear'))\n",
    "\n",
    "# Define your RL agent\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "dqn = DQNAgent(model=cnn_model, nb_actions=num_actions, memory=memory, nb_steps_warmup=1000,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "\n",
    "# Compile the agent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Train the agent\n",
    "dqn.fit(env, nb_steps=10000, visualize=True, verbose=1)\n",
    "\n",
    "# Test the trained agent\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
