{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU) está disponible en tu sistema.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verify if Cuda is available to use GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU) está disponible en tu sistema.\")\n",
    "else:\n",
    "    print(\"CUDA (GPU) no está disponible en tu sistema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI by reinforcement learning \n",
    "---\n",
    "This artificial intelligence will be able to learn to play and complete all levels of Geometry dash, using reinforcement learning. To complete this:\n",
    "1. Connect python with the game\n",
    "2. Define bounties and penalizations\n",
    "3. Use Reinforcement learning like Q-learning, Deep Q-learning or Proximal Policy Optimization\n",
    "4. Build a neuronal nerwork CNN? \n",
    "6. Training\n",
    "7. Eval and tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Connect Python with the Game\n",
    "---\n",
    "This step will require:\n",
    "1. Keyboard Emulation and Automation\n",
    "- Description: Simulates keystrokes to control the game.\n",
    "- Tools: Libraries such as pyautogui in Python can be used to simulate keystrokes.\n",
    "- Challenges: Can be slow and not always accurate.\n",
    "2. Use of Computer Vision Tools\n",
    "- Description: Uses computer vision to \"see\" the state of the game and makes decisions based on that.\n",
    "- Tools: Libraries such as OpenCV to process the visual output of the game.\n",
    "- Challenges: Requires image processing and can be computationally intensive.\n",
    "3. Creating a Custom Game Environment\n",
    "- Description: Create a simplified version of Geometry Dash that you can fully control.\n",
    "- Tools: Programming languages such as Python or JavaScript to create a basic version of the game.\n",
    "- Challenges: Requires time and effort to replicate the game mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def play_geometry_dash(model, capture_screen, process_image, get_reward, is_game_over, action_space, capture_rate=0.1):\n",
    "    \"\"\"\n",
    "    Juega Geometry Dash con una IA.\n",
    "\n",
    "    :param model: Modelo de IA para decidir acciones.\n",
    "    :param capture_screen: Función para capturar pantalla.\n",
    "    :param process_image: Función para procesar imágenes capturadas.\n",
    "    :param get_reward: Función para calcular la recompensa.\n",
    "    :param is_game_over: Función para determinar si el juego ha terminado.\n",
    "    :param action_space: Espacio de acciones posibles.\n",
    "    :param capture_rate: Tiempo entre capturas y acciones (en segundos).\n",
    "    \"\"\"\n",
    "\n",
    "    state = capture_screen()  # Captura el estado inicial\n",
    "    state = process_image(state)\n",
    "\n",
    "    while True:\n",
    "        action = model.predict_action(state)  # La IA elige una acción\n",
    "\n",
    "        if action == action_space['click']:\n",
    "            pyautogui.click()\n",
    "\n",
    "        time.sleep(capture_rate)  # Espera antes de capturar el nuevo estado\n",
    "\n",
    "        new_state = capture_screen()  # Captura el nuevo estado\n",
    "        new_state = process_image(new_state)\n",
    "\n",
    "        reward = get_reward(new_state)  # Calcula la recompensa\n",
    "        done = is_game_over(new_state)  # Verifica si el juego terminó\n",
    "\n",
    "        # Actualiza la memoria de repetición y entrena la IA\n",
    "        model.update_replay_memory(state, action, reward, new_state, done)\n",
    "        model.retrain()\n",
    "\n",
    "        if done:\n",
    "            break  # Fin del episodio\n",
    "\n",
    "        state = new_state  # Actualiza el estado\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture the game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (some auxiliary functions to cpt the game state)\n",
    "# do actions\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "\n",
    "# take a screenshot\n",
    "def capture_screen(region=None, resize_dim=(128, 128)):\n",
    "    # if region is None, will take the whole screen\n",
    "    screen = pyautogui.screenshot(region=region)\n",
    "    frame = np.array(screen)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, resize_dim)\n",
    "    return frame\n",
    "\n",
    "def perform_action(action):\n",
    "    screen = capture_screen()\n",
    "    processed = process_image(screen)\n",
    "    bounty = get_bounty(screen)\n",
    "    if action == 0:\n",
    "        pyautogui.click()\n",
    "    \n",
    "    if action == 2:\n",
    "        return process_image(processed), bounty, True\n",
    "\n",
    "    return process_image(processed), bounty, False\n",
    "\n",
    "def get_game_feedback():\n",
    "    return get_bounty(), process_image(capture_screen())   \n",
    "\n",
    "def process_image(image, stack_size=4):\n",
    "    # Normalize\n",
    "    processed_image = image / 255.0\n",
    "\n",
    "    # Init stack\n",
    "    if 'image_stack' not in globals():\n",
    "        global image_stack\n",
    "        image_stack = np.repeat(processed_image[..., np.newaxis], stack_size, axis=2)\n",
    "\n",
    "    # Update stack images\n",
    "    image_stack = np.append(image_stack[:, :, 1:], processed_image[..., np.newaxis], axis=2)\n",
    "\n",
    "    return image_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def start_capturing_game(model):\n",
    "    # Define the region to capt the game\n",
    "    region = (0, 0, 1920, 1080)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        screen = capture_screen(region)\n",
    "        state = process_image(screen)\n",
    "        # Predict the action to do\n",
    "        action = model.predict_action(state)\n",
    "\n",
    "        perform_action(action)\n",
    "\n",
    "        # obtain reward and state\n",
    "        reward, new_state = get_game_feedback() \n",
    "\n",
    "        # Update model\n",
    "        model.update(state, action, reward, new_state)\n",
    "\n",
    "        cv2.imshow(\"Game Capture\", screen)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Define bounties\n",
    "\n",
    "Now let's define the rewards, we will give 1 point for staying alive, and subtract when eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_green_cube(screen):\n",
    "    \"\"\"\n",
    "    Detects the presence of a green cube in an image (this will be our geometry skin).\n",
    "\n",
    "    :param image_path: current screenshot\n",
    "    :return: A tuple containing the percentage of green pixels and the mask image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to HSV color space\n",
    "    hsv_image = cv2.cvtColor(screen, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the range of green color in HSV\n",
    "    lower_green = np.array([40, 40, 40])\n",
    "    upper_green = np.array([80, 255, 255])\n",
    "\n",
    "    # Create a mask to detect green objects in the image\n",
    "    green_mask = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "\n",
    "    # Calculate the percentage of green pixels\n",
    "    green_pixels = np.count_nonzero(green_mask)\n",
    "    total_pixels = screen.shape[0] * screen.shape[1]\n",
    "    green_percentage = (green_pixels / total_pixels) * 100\n",
    "    \n",
    "    # Return the percentage of green pixels and the mask for further analysis if needed\n",
    "    return green_percentage, green_mask\n",
    "\n",
    "def detect_if_win(screen):\n",
    "    \"\"\"\n",
    "    Detects the presence of a green cube in an image (this will be our geometry skin).\n",
    "\n",
    "    :param screen: current screenshot as a numpy array\n",
    "    :return: A tuple containing the percentage of green pixels and the mask image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to HSV color space\n",
    "    hsv_image = cv2.cvtColor(screen, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the range of yellow color in HSV\n",
    "    lower_yellow = np.array([25, 100, 100])  # lower boundary of yellow hue\n",
    "    upper_yellow = np.array([35, 255, 255])  # upper boundary of yellow hue\n",
    "\n",
    "\n",
    "    # Create a mask to detect green objects in the image\n",
    "    green_mask = cv2.inRange(hsv_image, lower_yellow, upper_yellow)\n",
    "\n",
    "    # Calculate the percentage of green pixels\n",
    "    green_pixels = np.count_nonzero(green_mask)\n",
    "    total_pixels = screen.shape[0] * screen.shape[1]\n",
    "    green_percentage = (green_pixels / total_pixels) * 100\n",
    "\n",
    "    # Display the original image and the green mask\n",
    "    #cv2.imshow(\"Original Image\", screen)\n",
    "    #cv2.imshow(\"Green Mask\", green_mask)\n",
    "   # cv2.waitKey(0)  # Waits for a key press to close the windows\n",
    "   # cv2.destroyAllWindows()\n",
    "\n",
    "    # Return the percentage of green pixels and the mask for further analysis if needed\n",
    "    return green_percentage, green_mask\n",
    "\n",
    "## Seems working properly detecting the block\n",
    "def get_bounty(screen):\n",
    "    green_percentage, green_mask = detect_green_cube(screen)\n",
    "\n",
    "    if green_percentage > 0.35:\n",
    "        return 1 \n",
    "    else:\n",
    "        yellow_percetage, yellow_mask = detect_if_win(screen)\n",
    "        if yellow_percetage > 0.45 and yellow_percetage < 0.55:\n",
    "            return 100\n",
    "        return -10  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(5)\n",
    "#detect_if_win(capture_screen())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Use Reinforcement learning\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need a buffer of repetitions and some auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def choose_action(state, model, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        q_values = model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "def update_network(model, target_model, minibatch, gamma):\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target_q = reward\n",
    "        if not done:\n",
    "            target_q += gamma * np.amax(target_model.predict(next_state)[0])\n",
    "        target_q_values = model.predict(state)\n",
    "        target_q_values[0][action] = target_q\n",
    "\n",
    "        model.fit(state, target_q_values, epochs=1, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import L\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "base_model = MobileNetV2(input_shape=(128, 128, 3),\n",
    "                         include_top=False)\n",
    "def create_model():\n",
    "  return models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.7),\n",
    "    layers.Dense(102, activation='softmax')  # Oxford Flowers 102\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 5 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     32\u001b[0m     action \u001b[38;5;241m=\u001b[39m choose_action(state, model, epsilon)\n\u001b[1;32m---> 33\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43mperform_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m process_image(next_state)\n\u001b[0;32m     35\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m, in \u001b[0;36mperform_action\u001b[1;34m(action)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process_image(processed), bounty, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m)\u001b[49m, bounty, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 41\u001b[0m, in \u001b[0;36mprocess_image\u001b[1;34m(image, stack_size)\u001b[0m\n\u001b[0;32m     38\u001b[0m     image_stack \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(processed_image[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis], stack_size, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Update stack images\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m image_stack \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_stack\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_image\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_stack\n",
      "File \u001b[1;32me:\\AI_FOR_Geometry_dash\\.venv\\lib\\site-packages\\numpy\\lib\\function_base.py:5617\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5615\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[0;32m   5616\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 5 dimension(s)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Parámetros\n",
    "gamma = 0.99  # Discount \n",
    "epsilon = 1.0  # Epsilon to start epsilon-greedy politic\n",
    "epsilon_min = 0.01  # min Epsilon\n",
    "epsilon_decay = 0.995  # Factor de decaimiento de epsilon\n",
    "learning_rate = 0.001  # Learning Rate\n",
    "replay_buffer_capacity = 10000  # Max buffer capacity\n",
    "minibatch_size = 32  # Tamaño del minibatch\n",
    "num_episodes = 1000  # Número de episodios para entrenar\n",
    "num_actions = 2  # Definir el número de acciones posibles\n",
    "processed_image_shape = (128,128,3)  # Definir la forma de la imagen procesada\n",
    "\n",
    "# Construir la red neuronal y el modelo objetivo\n",
    "model = create_model()\n",
    "target_model = create_model()\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "target_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "\n",
    "time.sleep(5)  # Tiempo para preparar el juego\n",
    "\n",
    "# Buffer de repetición\n",
    "replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = process_image(capture_screen())\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state, model, epsilon)\n",
    "        next_state, reward, done = perform_action(action)\n",
    "        next_state = process_image(next_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        replay_buffer.store(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_buffer) > minibatch_size:\n",
    "            minibatch = replay_buffer.sample(minibatch_size)\n",
    "            update_network(model, target_model, minibatch, gamma)\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    print(f'Episodio: {episode}, Recompensa total: {total_reward}, Epsilon: {epsilon}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
